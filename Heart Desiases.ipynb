{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n",
    "\n",
    "Content\n",
    "\n",
    "Attribute Information: \n",
    "> 1. age \n",
    "> 2. sex \n",
    "> 3. chest pain type (4 values) \n",
    "> 4. resting blood pressure \n",
    "> 5. serum cholestoral in mg/dl \n",
    "> 6. fasting blood sugar > 120 mg/dl\n",
    "> 7. resting electrocardiographic results (values 0,1,2)\n",
    "> 8. maximum heart rate achieved \n",
    "> 9. exercise induced angina \n",
    "> 10. oldpeak = ST depression induced by exercise relative to rest \n",
    "> 11. the slope of the peak exercise ST segment \n",
    "> 12. number of major vessels (0-3) colored by flourosopy \n",
    "> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n",
    "\n",
    "To see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n",
    "\n",
    "Acknowledgements\n",
    "Creators: \n",
    "1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D. \n",
    "2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. \n",
    "3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D. \n",
    "4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "\n",
    "Donor: David W. Aha (aha '@' ics.uci.edu) (714) 856-8779\n",
    "\n",
    "Inspiration\n",
    "Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n",
    "\n",
    "See if you can find any other trends in heart data to predict certain cardiovascular events or find any clear indications of heart health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pandas.read_csv(\"heart.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[1:,0:-1].astype(float)\n",
    "Y = dataset[1:,-1]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 14)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.,  1.,  3., ...,  0.,  0.,  1.],\n",
       "       [37.,  1.,  2., ...,  0.,  0.,  2.],\n",
       "       [41.,  0.,  1., ...,  2.,  0.,  2.],\n",
       "       ...,\n",
       "       [68.,  1.,  0., ...,  1.,  2.,  3.],\n",
       "       [57.,  1.,  0., ...,  1.,  1.,  3.],\n",
       "       [57.,  0.,  1., ...,  1.,  1.,  2.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode class values as integers\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(Y)\n",
    "# encoded_Y = encoder.transform(Y)\n",
    "# # convert integers to dummy variables (i.e. one hot encoded)\n",
    "# dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(60, activation = 'relu', input_shape=(X.shape[1],)))\n",
    "model.add(layers.Dense(1, activation = 'relu'))\n",
    "        \n",
    "model.compile(optimizer = 'Adam',loss= 'binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "303/303 [==============================] - 2s 6ms/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 0s 191us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 0s 244us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 0s 224us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 0s 211us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 0s 347us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 0s 234us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 0s 241us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 0s 208us/step - loss: 8.7772 - acc: 0.4554\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 0s 284us/step - loss: 8.7772 - acc: 0.4554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf5bdf98>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=10, batch_size =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def New_Model():\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(13, activation = \"relu\", input_shape =(13,)))\n",
    "    model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer = \"Adam\",\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 56.11% (12.56%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=New_Model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Second_Model():\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(200, activation = 'relu', input_shape=(13,)))\n",
    "    model.add(layers.Dense(100, activation = 'relu'))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'Adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second_Results: 66.13% (12.03%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=Second_Model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Second_Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize data mathod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"heart.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[1:,0:-1].astype(float)\n",
    "Y = dataset[1:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X.mean(axis=0)\n",
    "X -= mean\n",
    "std = X.std(axis=0)\n",
    "X /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Model():\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(100, activation = 'relu', input_shape =(13,)))\n",
    "    model.add(layers.Dense(50, activation = 'relu'))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'Adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics =['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize_Model_Results: 78.89% (7.58%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=Normalize_Model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Normalize_Model_Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Func_API():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "\n",
    "    inputs=Input(shape=(13,))\n",
    "    x1 = Dense(80, activation = 'relu')(inputs)\n",
    "    x2 = Dense(60, activation = 'relu')(x1)\n",
    "    outputs = Dense(1, activation= 'sigmoid')(x2)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss= 'binary_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Func_API_Results: 79.61% (6.74%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=Func_API, epochs=28, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Func_API_Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Subclassing():\n",
    "    import keras\n",
    "    from keras import layers\n",
    "    from keras.layers import Input, Dense\n",
    "\n",
    "    inputs = Input(shape=(13,))\n",
    "\n",
    "    class MyModel(keras.Model):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.dense1 = layers.Dense(100, activation = 'relu')\n",
    "            self.dense2 = layers.Dense(60, activation = 'relu')\n",
    "            self.dense3 = layers.Dense(1 , activation = 'sigmoid')\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x1 = self.dense1(inputs)\n",
    "            x2 = self.dense2(x1)\n",
    "            return self.dense3(x2)\n",
    "\n",
    "    model = MyModel()\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "272/272 [==============================] - 8s 29ms/step - loss: 0.5403 - acc: 0.7426\n",
      "Epoch 2/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.3797 - acc: 0.8382\n",
      "Epoch 3/60\n",
      "272/272 [==============================] - 0s 989us/step - loss: 0.3278 - acc: 0.8566\n",
      "Epoch 4/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.3019 - acc: 0.8640\n",
      "Epoch 5/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2892 - acc: 0.8713\n",
      "Epoch 6/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2747 - acc: 0.8787\n",
      "Epoch 7/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2662 - acc: 0.8787\n",
      "Epoch 8/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2466 - acc: 0.8824\n",
      "Epoch 9/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2422 - acc: 0.8934\n",
      "Epoch 10/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2291 - acc: 0.9081\n",
      "Epoch 11/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2223 - acc: 0.9007\n",
      "Epoch 12/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2109 - acc: 0.9118\n",
      "Epoch 13/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2020 - acc: 0.9191\n",
      "Epoch 14/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1919 - acc: 0.9191\n",
      "Epoch 15/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1806 - acc: 0.9338A: 0s - loss: 0.1659 - acc: 0\n",
      "Epoch 16/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1740 - acc: 0.9265\n",
      "Epoch 17/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1618 - acc: 0.9375\n",
      "Epoch 18/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1549 - acc: 0.9375\n",
      "Epoch 19/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1458 - acc: 0.9449\n",
      "Epoch 20/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1415 - acc: 0.9449\n",
      "Epoch 21/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1343 - acc: 0.9485A: 0s - loss: 0.0919 - acc: \n",
      "Epoch 22/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1188 - acc: 0.9559\n",
      "Epoch 23/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1166 - acc: 0.9596\n",
      "Epoch 24/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.1050 - acc: 0.9743\n",
      "Epoch 25/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0960 - acc: 0.9743\n",
      "Epoch 26/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0893 - acc: 0.9816\n",
      "Epoch 27/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0869 - acc: 0.9779\n",
      "Epoch 28/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0766 - acc: 0.9890\n",
      "Epoch 29/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0720 - acc: 0.9890\n",
      "Epoch 30/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0668 - acc: 0.9853\n",
      "Epoch 31/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0600 - acc: 0.9890\n",
      "Epoch 32/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0530 - acc: 0.9853\n",
      "Epoch 33/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0517 - acc: 0.9890\n",
      "Epoch 34/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0427 - acc: 0.9926\n",
      "Epoch 35/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0473 - acc: 0.9890\n",
      "Epoch 36/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0374 - acc: 0.9926\n",
      "Epoch 37/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0325 - acc: 0.9926\n",
      "Epoch 38/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0316 - acc: 0.9926\n",
      "Epoch 39/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0304 - acc: 0.9853\n",
      "Epoch 40/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0271 - acc: 0.9926\n",
      "Epoch 41/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0232 - acc: 0.9890\n",
      "Epoch 42/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0244 - acc: 0.9890\n",
      "Epoch 43/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0184 - acc: 0.9926\n",
      "Epoch 44/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0187 - acc: 0.9963\n",
      "Epoch 45/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0154 - acc: 0.9963\n",
      "Epoch 46/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0147 - acc: 0.9963\n",
      "Epoch 47/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0142 - acc: 0.992 - 0s 2ms/step - loss: 0.0139 - acc: 0.9926\n",
      "Epoch 48/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0119 - acc: 0.9963\n",
      "Epoch 49/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0105 - acc: 0.9963\n",
      "Epoch 50/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0090 - acc: 0.9963\n",
      "Epoch 52/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0077 - acc: 0.9963\n",
      "Epoch 53/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0042 - acc: 1.000 - 1s 2ms/step - loss: 0.0088 - acc: 0.9963\n",
      "Epoch 54/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 55/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0019 - acc: 1.0000\n",
      "31/31 [==============================] - 1s 41ms/step\n",
      "Epoch 1/60\n",
      "272/272 [==============================] - 3s 13ms/step - loss: 0.5451 - acc: 0.7537\n",
      "Epoch 2/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.3725 - acc: 0.8493A: 0s - loss: 0.4049 - acc: \n",
      "Epoch 3/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.3293 - acc: 0.8676\n",
      "Epoch 4/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.3067 - acc: 0.8750\n",
      "Epoch 5/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2955 - acc: 0.8750\n",
      "Epoch 6/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2800 - acc: 0.8897\n",
      "Epoch 7/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2687 - acc: 0.8860\n",
      "Epoch 8/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2604 - acc: 0.8934\n",
      "Epoch 9/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2397 - acc: 0.9154\n",
      "Epoch 10/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2319 - acc: 0.9191\n",
      "Epoch 11/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2167 - acc: 0.9118\n",
      "Epoch 12/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2036 - acc: 0.9118\n",
      "Epoch 13/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1940 - acc: 0.9265\n",
      "Epoch 14/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1925 - acc: 0.9228\n",
      "Epoch 15/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1765 - acc: 0.9301\n",
      "Epoch 16/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1668 - acc: 0.9338\n",
      "Epoch 17/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1550 - acc: 0.9375\n",
      "Epoch 18/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1490 - acc: 0.9449\n",
      "Epoch 19/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1363 - acc: 0.9522\n",
      "Epoch 20/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1279 - acc: 0.9485A: 0s - loss: 0.1326 - acc: 0.944\n",
      "Epoch 21/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1259 - acc: 0.9596A: 0s - loss: 0.1325 - acc: 0.956\n",
      "Epoch 22/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1114 - acc: 0.9669\n",
      "Epoch 23/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1106 - acc: 0.9559\n",
      "Epoch 24/60\n",
      "272/272 [==============================] - 0s 820us/step - loss: 0.0990 - acc: 0.9743\n",
      "Epoch 25/60\n",
      "272/272 [==============================] - 0s 849us/step - loss: 0.0896 - acc: 0.9706\n",
      "Epoch 26/60\n",
      "272/272 [==============================] - 0s 938us/step - loss: 0.0856 - acc: 0.9743\n",
      "Epoch 27/60\n",
      "272/272 [==============================] - 0s 886us/step - loss: 0.0830 - acc: 0.9743\n",
      "Epoch 28/60\n",
      "272/272 [==============================] - 0s 842us/step - loss: 0.0751 - acc: 0.9779 0s - loss: 0.0570 - acc: 0.98\n",
      "Epoch 29/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0643 - acc: 0.9743\n",
      "Epoch 30/60\n",
      "272/272 [==============================] - 0s 875us/step - loss: 0.0594 - acc: 0.9816\n",
      "Epoch 31/60\n",
      "272/272 [==============================] - 0s 897us/step - loss: 0.0565 - acc: 0.9779\n",
      "Epoch 32/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0529 - acc: 0.987 - 0s 934us/step - loss: 0.0515 - acc: 0.9890\n",
      "Epoch 33/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0499 - acc: 0.9816\n",
      "Epoch 34/60\n",
      "272/272 [==============================] - 0s 919us/step - loss: 0.0443 - acc: 0.9853\n",
      "Epoch 35/60\n",
      "272/272 [==============================] - 0s 779us/step - loss: 0.0369 - acc: 0.9890\n",
      "Epoch 36/60\n",
      "272/272 [==============================] - 0s 915us/step - loss: 0.0378 - acc: 0.9853\n",
      "Epoch 37/60\n",
      "272/272 [==============================] - 0s 890us/step - loss: 0.0321 - acc: 0.9963\n",
      "Epoch 38/60\n",
      "272/272 [==============================] - 0s 890us/step - loss: 0.0290 - acc: 0.9963\n",
      "Epoch 39/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0331 - acc: 0.9890\n",
      "Epoch 40/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0237 - acc: 0.9963\n",
      "Epoch 41/60\n",
      "272/272 [==============================] - 0s 879us/step - loss: 0.0271 - acc: 0.9926\n",
      "Epoch 42/60\n",
      "272/272 [==============================] - 0s 952us/step - loss: 0.0215 - acc: 0.9926ETA: 0s - loss: 0.0123 - acc: 1.0\n",
      "Epoch 43/60\n",
      "272/272 [==============================] - 0s 871us/step - loss: 0.0204 - acc: 0.9963\n",
      "Epoch 44/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0157 - acc: 0.9963\n",
      "Epoch 45/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0191 - acc: 0.9963 ETA: 0s - loss: 0.0106 - acc: 1.0\n",
      "Epoch 46/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0140 - acc: 0.9963\n",
      "Epoch 47/60\n",
      "272/272 [==============================] - 0s 963us/step - loss: 0.0135 - acc: 0.9963\n",
      "Epoch 48/60\n",
      "272/272 [==============================] - 0s 927us/step - loss: 0.0154 - acc: 0.9963\n",
      "Epoch 49/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 1.0000- ETA: 0s - loss: 0.0018 - acc: 1.000\n",
      "Epoch 51/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0149 - acc: 0.9926\n",
      "Epoch 53/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "272/272 [==============================] - 0s 967us/step - loss: 0.0055 - acc: 1.0000 0s - loss: 0.0087 - acc: 1.\n",
      "Epoch 55/60\n",
      "272/272 [==============================] - 0s 989us/step - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0057 - acc: 1.0000\n",
      "31/31 [==============================] - 1s 42ms/step\n",
      "Epoch 1/60\n",
      "272/272 [==============================] - 4s 15ms/step - loss: 0.4701 - acc: 0.8051\n",
      "Epoch 2/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.3509 - acc: 0.8529\n",
      "Epoch 3/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.3259 - acc: 0.8640\n",
      "Epoch 4/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.3004 - acc: 0.8824\n",
      "Epoch 5/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.2810 - acc: 0.8676\n",
      "Epoch 6/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.2708 - acc: 0.8787\n",
      "Epoch 7/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2493 - acc: 0.8824\n",
      "Epoch 8/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2386 - acc: 0.9081\n",
      "Epoch 9/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2270 - acc: 0.9081A: 0s - loss: 0.2001 - acc\n",
      "Epoch 10/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2153 - acc: 0.9081\n",
      "Epoch 11/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1960 - acc: 0.9265\n",
      "Epoch 12/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1878 - acc: 0.9338\n",
      "Epoch 13/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1715 - acc: 0.9449\n",
      "Epoch 14/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1577 - acc: 0.9338\n",
      "Epoch 15/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1502 - acc: 0.9338\n",
      "Epoch 16/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.1337 - acc: 0.9522\n",
      "Epoch 17/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1234 - acc: 0.9485\n",
      "Epoch 18/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1136 - acc: 0.9596\n",
      "Epoch 19/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1006 - acc: 0.9743\n",
      "Epoch 20/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0927 - acc: 0.9669A: 0s - loss: 0.1096 - ac\n",
      "Epoch 21/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0800 - acc: 0.9816\n",
      "Epoch 22/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0769 - acc: 0.9743\n",
      "Epoch 23/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0665 - acc: 0.9816\n",
      "Epoch 24/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0583 - acc: 0.9816\n",
      "Epoch 25/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0491 - acc: 0.9926\n",
      "Epoch 26/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0493 - acc: 0.9890A: 0s - loss: 0.0261 - a\n",
      "Epoch 27/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0361 - acc: 1.0000\n",
      "Epoch 28/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 29/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0269 - acc: 0.9963A: 0s - loss: 0.0134 - ac\n",
      "Epoch 30/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0215 - acc: 1.0000\n",
      "Epoch 31/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 32/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 33/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 34/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0139 - acc: 0.9963\n",
      "Epoch 35/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 36/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 37/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 38/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 39/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0037 - acc: 1.0000A: 0s - loss: 0.0026 - acc:\n",
      "Epoch 40/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 41/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 42/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 43/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 44/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.0000 - 0s 1ms/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 45/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 7.1911e-04 - acc: 1.0000\n",
      "Epoch 46/60\n",
      "272/272 [==============================] - 0s 978us/step - loss: 6.6387e-04 - acc: 1.0000\n",
      "Epoch 47/60\n",
      "272/272 [==============================] - 0s 993us/step - loss: 6.5501e-04 - acc: 1.0000\n",
      "Epoch 48/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 3.0727e-04 - acc: 1.0000\n",
      "Epoch 49/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 3.3824e-04 - acc: 1.0000A: 0s - loss: 2.9026e-04 - acc: 1\n",
      "Epoch 50/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 2.0319e-04 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 2.0370e-04 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 9.0863e-05 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 1.0960e-04 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 8.8800e-05 - acc: 1.0000\n",
      "Epoch 55/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 4.7364e-05 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 4.1293e-05 - acc: 1.0000A: 0s - loss: 2.1358e-05 - acc: 1.\n",
      "Epoch 57/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 2.2518e-05 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 2.2557e-05 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 2.0477e-05 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 1.9196e-05 - acc: 1.0000\n",
      "31/31 [==============================] - 1s 39ms/step\n",
      "Epoch 1/60\n",
      "272/272 [==============================] - 4s 13ms/step - loss: 0.4950 - acc: 0.8015\n",
      "Epoch 2/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.3795 - acc: 0.8382A: 0s - loss: 0.3653 - acc: 0.8\n",
      "Epoch 3/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.3521 - acc: 0.8640\n",
      "Epoch 4/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.3263 - acc: 0.8566\n",
      "Epoch 5/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.3130 - acc: 0.8860\n",
      "Epoch 6/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2992 - acc: 0.8860\n",
      "Epoch 7/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2870 - acc: 0.8934\n",
      "Epoch 8/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.2706 - acc: 0.8934\n",
      "Epoch 9/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2627 - acc: 0.9007\n",
      "Epoch 10/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.2436 - acc: 0.8971\n",
      "Epoch 11/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2346 - acc: 0.9044\n",
      "Epoch 12/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.2248 - acc: 0.9081\n",
      "Epoch 13/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.2059 - acc: 0.9154\n",
      "Epoch 14/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1986 - acc: 0.9265\n",
      "Epoch 15/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1897 - acc: 0.9265\n",
      "Epoch 16/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.1758 - acc: 0.9375\n",
      "Epoch 17/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1620 - acc: 0.9375\n",
      "Epoch 18/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1572 - acc: 0.9485\n",
      "Epoch 19/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1461 - acc: 0.9449\n",
      "Epoch 20/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.1343 - acc: 0.9632\n",
      "Epoch 21/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1285 - acc: 0.9596\n",
      "Epoch 22/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.1191 - acc: 0.9632\n",
      "Epoch 23/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1055 - acc: 0.9706\n",
      "Epoch 24/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.1015 - acc: 0.9779\n",
      "Epoch 25/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0894 - acc: 0.9890\n",
      "Epoch 26/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0814 - acc: 0.9811- ETA: 0s - loss: 0.0625 - a - 1s 2ms/step - loss: 0.0847 - acc: 0.9779\n",
      "Epoch 27/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0767 - acc: 0.9853\n",
      "Epoch 28/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0765 - acc: 0.9779\n",
      "Epoch 29/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0626 - acc: 0.9890\n",
      "Epoch 30/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0606 - acc: 0.9779\n",
      "Epoch 31/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0564 - acc: 0.9853\n",
      "Epoch 32/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0468 - acc: 0.9926\n",
      "Epoch 33/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0478 - acc: 0.9853\n",
      "Epoch 34/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0395 - acc: 0.9926\n",
      "Epoch 35/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0400 - acc: 0.9963\n",
      "Epoch 36/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0365 - acc: 0.9926\n",
      "Epoch 37/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0287 - acc: 0.9963\n",
      "Epoch 38/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0288 - acc: 0.9963\n",
      "Epoch 39/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0242 - acc: 0.9926\n",
      "Epoch 40/60\n",
      "272/272 [==============================] - 1s 5ms/step - loss: 0.0257 - acc: 0.9963\n",
      "Epoch 41/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0223 - acc: 0.9963\n",
      "Epoch 42/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0166 - acc: 0.9963\n",
      "Epoch 43/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0193 - acc: 0.9926\n",
      "Epoch 44/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 45/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0159 - acc: 0.9963\n",
      "Epoch 46/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0130 - acc: 0.9963\n",
      "Epoch 47/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0107 - acc: 0.9963\n",
      "Epoch 48/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0103 - acc: 0.9963\n",
      "Epoch 49/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0080 - acc: 0.9963\n",
      "Epoch 52/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.0072 - acc: 0.9963\n",
      "Epoch 54/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0056 - acc: 0.9963A: 0s - loss: 0.0031 - acc: \n",
      "Epoch 55/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0048 - acc: 0.9963\n",
      "Epoch 59/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0019 - acc: 1.0000\n",
      "31/31 [==============================] - 1s 40ms/step\n",
      "Epoch 1/60\n",
      "272/272 [==============================] - 5s 17ms/step - loss: 0.5498 - acc: 0.7610: 17s \n",
      "Epoch 2/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.3873 - acc: 0.8346\n",
      "Epoch 3/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.3471 - acc: 0.8419A: 0s - loss: 0.4075\n",
      "Epoch 4/60\n",
      "272/272 [==============================] - 1s 5ms/step - loss: 0.3283 - acc: 0.8566\n",
      "Epoch 5/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 2ms/step - loss: 0.3031 - acc: 0.8750\n",
      "Epoch 6/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2871 - acc: 0.8824\n",
      "Epoch 7/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2714 - acc: 0.8897\n",
      "Epoch 8/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2572 - acc: 0.9118\n",
      "Epoch 9/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2459 - acc: 0.9081\n",
      "Epoch 10/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2316 - acc: 0.9118\n",
      "Epoch 11/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2202 - acc: 0.9191\n",
      "Epoch 12/60\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 0.2063 - acc: 0.9301\n",
      "Epoch 13/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1950 - acc: 0.9191\n",
      "Epoch 14/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1846 - acc: 0.9375\n",
      "Epoch 15/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1719 - acc: 0.9412\n",
      "Epoch 16/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1618 - acc: 0.9449\n",
      "Epoch 17/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1518 - acc: 0.9485\n",
      "Epoch 18/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.1323 - acc: 0.975 - 0s 1ms/step - loss: 0.1403 - acc: 0.9669\n",
      "Epoch 19/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1328 - acc: 0.9559\n",
      "Epoch 20/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1251 - acc: 0.9596\n",
      "Epoch 21/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.1163 - acc: 0.9669\n",
      "Epoch 22/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.1089 - acc: 0.9669\n",
      "Epoch 23/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0958 - acc: 0.9743\n",
      "Epoch 24/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0917 - acc: 0.9669A: 0s - loss: 0.0507 - a\n",
      "Epoch 25/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0800 - acc: 0.9816\n",
      "Epoch 26/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0754 - acc: 0.9816\n",
      "Epoch 27/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0611 - acc: 0.9853\n",
      "Epoch 28/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0654 - acc: 0.9816\n",
      "Epoch 29/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0579 - acc: 0.9853A: 0s - loss: 0.0409 - acc\n",
      "Epoch 30/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0507 - acc: 0.9853\n",
      "Epoch 31/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0452 - acc: 0.9890\n",
      "Epoch 32/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0422 - acc: 0.9890\n",
      "Epoch 33/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0348 - acc: 0.9926\n",
      "Epoch 34/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0319 - acc: 0.9963\n",
      "Epoch 35/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0267 - acc: 0.9963\n",
      "Epoch 36/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0278 - acc: 0.9963\n",
      "Epoch 37/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.9963\n",
      "Epoch 38/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0220 - acc: 0.9963\n",
      "Epoch 39/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0170 - acc: 0.9963\n",
      "Epoch 40/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0148 - acc: 0.9963\n",
      "Epoch 41/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 42/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0124 - acc: 0.9963\n",
      "Epoch 43/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0105 - acc: 1.0000A: 0s - loss: 0.0069 - ac\n",
      "Epoch 44/60\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 0.0096 - acc: 0.9963- ETA: 1s - loss: 0.008\n",
      "Epoch 45/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 46/60\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 47/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 48/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 49/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0038 - acc: 1.000 - 0s 2ms/step - loss: 0.0067 - acc: 0.9963\n",
      "Epoch 51/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0071 - acc: 0.9963- ETA: 0s - loss: 0.0249 - acc\n",
      "Epoch 55/60\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9962 - 0s 2ms/step - loss: 0.0078 - acc: 0.9963\n",
      "Epoch 57/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 8.3612e-04 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 0.0062 - acc: 0.9963\n",
      "Epoch 60/60\n",
      "272/272 [==============================] - 1s 2ms/step - loss: 7.4562e-04 - acc: 1.0000\n",
      "31/31 [==============================] - 1s 42ms/step\n",
      "Epoch 1/60\n",
      "273/273 [==============================] - 4s 16ms/step - loss: 0.5192 - acc: 0.7582\n",
      "Epoch 2/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.3845 - acc: 0.8205\n",
      "Epoch 3/60\n",
      "273/273 [==============================] - ETA: 0s - loss: 0.3572 - acc: 0.845 - 1s 4ms/step - loss: 0.3500 - acc: 0.8498\n",
      "Epoch 4/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.3357 - acc: 0.8498\n",
      "Epoch 5/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.3145 - acc: 0.8718A: 1s - loss: 0.3\n",
      "Epoch 6/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2998 - acc: 0.8755\n",
      "Epoch 7/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.2868 - acc: 0.8828\n",
      "Epoch 8/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.2684 - acc: 0.8938\n",
      "Epoch 9/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.2615 - acc: 0.9121\n",
      "Epoch 10/60\n",
      "273/273 [==============================] - 2s 6ms/step - loss: 0.2486 - acc: 0.9048\n",
      "Epoch 11/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2368 - acc: 0.9158\n",
      "Epoch 12/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2202 - acc: 0.9084\n",
      "Epoch 13/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.2128 - acc: 0.9158\n",
      "Epoch 14/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.1957 - acc: 0.9158\n",
      "Epoch 15/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.1899 - acc: 0.9231\n",
      "Epoch 16/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.1805 - acc: 0.9231\n",
      "Epoch 17/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.1664 - acc: 0.9414\n",
      "Epoch 18/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.1602 - acc: 0.9341\n",
      "Epoch 19/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.1470 - acc: 0.9414A: 0s - loss: 0.1456 - acc: 0.94\n",
      "Epoch 20/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1397 - acc: 0.9451\n",
      "Epoch 21/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1277 - acc: 0.9524A: 0s - loss: 0.1369 - acc: 0.95\n",
      "Epoch 22/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1213 - acc: 0.9451\n",
      "Epoch 23/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.1140 - acc: 0.9560\n",
      "Epoch 24/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.1046 - acc: 0.9487\n",
      "Epoch 25/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.1004 - acc: 0.9597\n",
      "Epoch 26/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0897 - acc: 0.9707\n",
      "Epoch 27/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0878 - acc: 0.9744A: 0s - loss: 0.0813 - acc: 0.\n",
      "Epoch 28/60\n",
      "273/273 [==============================] - 0s 967us/step - loss: 0.0791 - acc: 0.9780\n",
      "Epoch 29/60\n",
      "273/273 [==============================] - 0s 938us/step - loss: 0.0727 - acc: 0.9744\n",
      "Epoch 30/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0658 - acc: 0.9780\n",
      "Epoch 31/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0661 - acc: 0.9817\n",
      "Epoch 32/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0574 - acc: 0.9890\n",
      "Epoch 33/60\n",
      "273/273 [==============================] - 0s 952us/step - loss: 0.0525 - acc: 0.9817\n",
      "Epoch 34/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0460 - acc: 0.9890\n",
      "Epoch 35/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0442 - acc: 0.9927\n",
      "Epoch 36/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0380 - acc: 0.9927\n",
      "Epoch 37/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0379 - acc: 0.9927\n",
      "Epoch 38/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0307 - acc: 0.9890\n",
      "Epoch 39/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0280 - acc: 0.9927\n",
      "Epoch 40/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0284 - acc: 0.9890\n",
      "Epoch 41/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0239 - acc: 0.9963A: 0s - loss: 0.0134 - acc: 1\n",
      "Epoch 42/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 43/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0248 - acc: 0.9963A: 0s - loss: 0.0128 - acc: 1\n",
      "Epoch 44/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0163 - acc: 0.9963\n",
      "Epoch 45/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0181 - acc: 0.9963\n",
      "Epoch 46/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0168 - acc: 0.9927\n",
      "Epoch 47/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0125 - acc: 0.9927\n",
      "Epoch 48/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0146 - acc: 0.9963\n",
      "Epoch 49/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0099 - acc: 0.9963 ETA: 0s - loss: 0.0066 - acc: 1.\n",
      "Epoch 52/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.9927\n",
      "Epoch 53/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0092 - acc: 0.9963\n",
      "Epoch 54/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 55/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "273/273 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.995 - 0s 1ms/step - loss: 0.0060 - acc: 0.9963\n",
      "Epoch 60/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0035 - acc: 1.0000\n",
      "30/30 [==============================] - 1s 44ms/step\n",
      "Epoch 1/60\n",
      "273/273 [==============================] - 4s 14ms/step - loss: 0.5299 - acc: 0.7656\n",
      "Epoch 2/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.3663 - acc: 0.8571\n",
      "Epoch 3/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.3267 - acc: 0.8645\n",
      "Epoch 4/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.3073 - acc: 0.9048\n",
      "Epoch 5/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.2912 - acc: 0.9011\n",
      "Epoch 6/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.2750 - acc: 0.8974\n",
      "Epoch 7/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.2587 - acc: 0.9011\n",
      "Epoch 8/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2539 - acc: 0.8974\n",
      "Epoch 9/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2331 - acc: 0.8974\n",
      "Epoch 10/60\n",
      "273/273 [==============================] - ETA: 0s - loss: 0.2307 - acc: 0.8981- ETA: 0s - loss: 0.2598 - acc:  - 1s 2ms/step - loss: 0.2250 - acc: 0.9011\n",
      "Epoch 11/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2166 - acc: 0.9158\n",
      "Epoch 12/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2072 - acc: 0.9194\n",
      "Epoch 13/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.1952 - acc: 0.9267\n",
      "Epoch 14/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.1822 - acc: 0.9231\n",
      "Epoch 15/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1733 - acc: 0.9377\n",
      "Epoch 16/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1630 - acc: 0.9414\n",
      "Epoch 17/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1526 - acc: 0.9487\n",
      "Epoch 18/60\n",
      "273/273 [==============================] - ETA: 0s - loss: 0.1449 - acc: 0.947 - 0s 2ms/step - loss: 0.1424 - acc: 0.9414\n",
      "Epoch 19/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1346 - acc: 0.9487\n",
      "Epoch 20/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1212 - acc: 0.9597\n",
      "Epoch 21/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1170 - acc: 0.9524\n",
      "Epoch 22/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1045 - acc: 0.9670\n",
      "Epoch 23/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1041 - acc: 0.9780\n",
      "Epoch 24/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0958 - acc: 0.9707\n",
      "Epoch 25/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0881 - acc: 0.9744\n",
      "Epoch 26/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0704 - acc: 0.9853\n",
      "Epoch 27/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0706 - acc: 0.9853\n",
      "Epoch 28/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0665 - acc: 0.9927\n",
      "Epoch 29/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0630 - acc: 0.9890\n",
      "Epoch 30/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0567 - acc: 0.9890\n",
      "Epoch 31/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0522 - acc: 0.9963\n",
      "Epoch 32/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0466 - acc: 0.9927\n",
      "Epoch 33/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0429 - acc: 0.9927\n",
      "Epoch 34/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 35/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0354 - acc: 0.9927\n",
      "Epoch 36/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0331 - acc: 0.9963\n",
      "Epoch 37/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0246 - acc: 0.9963\n",
      "Epoch 38/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0351 - acc: 0.9927\n",
      "Epoch 39/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.9963\n",
      "Epoch 40/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0239 - acc: 0.9927\n",
      "Epoch 41/60\n",
      "273/273 [==============================] - 2s 6ms/step - loss: 0.0187 - acc: 0.9963\n",
      "Epoch 42/60\n",
      "273/273 [==============================] - 2s 6ms/step - loss: 0.0203 - acc: 0.9963\n",
      "Epoch 43/60\n",
      "273/273 [==============================] - 2s 7ms/step - loss: 0.0152 - acc: 0.9963\n",
      "Epoch 44/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.0144 - acc: 0.9927\n",
      "Epoch 45/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0172 - acc: 0.9963\n",
      "Epoch 46/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0173 - acc: 0.9927\n",
      "Epoch 47/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 48/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0140 - acc: 0.9963\n",
      "Epoch 49/60\n",
      "273/273 [==============================] - 0s 923us/step - loss: 0.0092 - acc: 0.9963\n",
      "Epoch 50/60\n",
      "273/273 [==============================] - 0s 846us/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "273/273 [==============================] - 0s 956us/step - loss: 0.0084 - acc: 0.9963\n",
      "Epoch 52/60\n",
      "273/273 [==============================] - 0s 879us/step - loss: 0.0084 - acc: 0.9963\n",
      "Epoch 53/60\n",
      "273/273 [==============================] - 0s 919us/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "273/273 [==============================] - 0s 927us/step - loss: 0.0073 - acc: 0.9963\n",
      "Epoch 55/60\n",
      "273/273 [==============================] - 0s 982us/step - loss: 0.0050 - acc: 0.9963\n",
      "Epoch 56/60\n",
      "273/273 [==============================] - 0s 941us/step - loss: 0.0056 - acc: 0.9963\n",
      "Epoch 57/60\n",
      "273/273 [==============================] - 0s 897us/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "273/273 [==============================] - 0s 868us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "273/273 [==============================] - 0s 879us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.0072 - acc: 0.9963\n",
      "30/30 [==============================] - 1s 45ms/step\n",
      "Epoch 1/60\n",
      "273/273 [==============================] - 4s 14ms/step - loss: 0.4814 - acc: 0.8022\n",
      "Epoch 2/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.3703 - acc: 0.8352\n",
      "Epoch 3/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.3391 - acc: 0.8571\n",
      "Epoch 4/60\n",
      "273/273 [==============================] - 0s 1ms/step - loss: 0.3200 - acc: 0.8571\n",
      "Epoch 5/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.3075 - acc: 0.8755\n",
      "Epoch 6/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.2918 - acc: 0.8755\n",
      "Epoch 7/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2772 - acc: 0.8864A: 0s - loss: 0.3518 - ac\n",
      "Epoch 8/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2682 - acc: 0.8974\n",
      "Epoch 9/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2548 - acc: 0.8974\n",
      "Epoch 10/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.2435 - acc: 0.9084\n",
      "Epoch 11/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2281 - acc: 0.9121\n",
      "Epoch 12/60\n",
      "273/273 [==============================] - ETA: 0s - loss: 0.2266 - acc: 0.905 - 1s 2ms/step - loss: 0.2228 - acc: 0.9084\n",
      "Epoch 13/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.2099 - acc: 0.9194\n",
      "Epoch 14/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1971 - acc: 0.9231\n",
      "Epoch 15/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1848 - acc: 0.9341\n",
      "Epoch 16/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1745 - acc: 0.9304\n",
      "Epoch 17/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1638 - acc: 0.9267\n",
      "Epoch 18/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1546 - acc: 0.9524\n",
      "Epoch 19/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1389 - acc: 0.9560\n",
      "Epoch 20/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1363 - acc: 0.9560\n",
      "Epoch 21/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1241 - acc: 0.9524\n",
      "Epoch 22/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1156 - acc: 0.9670\n",
      "Epoch 23/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.1054 - acc: 0.9744\n",
      "Epoch 24/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.1018 - acc: 0.9744\n",
      "Epoch 25/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0882 - acc: 0.9853\n",
      "Epoch 26/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0873 - acc: 0.9707\n",
      "Epoch 27/60\n",
      "273/273 [==============================] - ETA: 0s - loss: 0.0734 - acc: 0.984 - 1s 3ms/step - loss: 0.0727 - acc: 0.9853\n",
      "Epoch 28/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0700 - acc: 0.9817\n",
      "Epoch 29/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0634 - acc: 0.9817\n",
      "Epoch 30/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0605 - acc: 0.9817\n",
      "Epoch 31/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0519 - acc: 0.9890\n",
      "Epoch 32/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0474 - acc: 0.9890\n",
      "Epoch 33/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0477 - acc: 0.9853\n",
      "Epoch 34/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0383 - acc: 0.9927\n",
      "Epoch 35/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0393 - acc: 0.9927\n",
      "Epoch 36/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0303 - acc: 0.9963\n",
      "Epoch 37/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 38/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0244 - acc: 0.9927\n",
      "Epoch 39/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0242 - acc: 0.9963\n",
      "Epoch 40/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 41/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0176 - acc: 0.9963\n",
      "Epoch 42/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 43/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 44/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 45/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.0144 - acc: 0.9963\n",
      "Epoch 46/60\n",
      "273/273 [==============================] - 0s 2ms/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 47/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 48/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 49/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0057 - acc: 0.9963\n",
      "Epoch 55/60\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "273/273 [==============================] - 1s 2ms/step - loss: 0.0024 - acc: 1.0000- ETA: 1s - loss: 4.7826e-04 - a\n",
      "Epoch 57/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "273/273 [==============================] - 2s 6ms/step - loss: 0.0025 - acc: 1.0000\n",
      "30/30 [==============================] - 1s 47ms/step\n",
      "Epoch 1/60\n",
      "274/274 [==============================] - 5s 17ms/step - loss: 0.5455 - acc: 0.7336\n",
      "Epoch 2/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.3861 - acc: 0.8248\n",
      "Epoch 3/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.3448 - acc: 0.8394A: 0s - loss: 0.3465 - acc: 0.83\n",
      "Epoch 4/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.3248 - acc: 0.8650\n",
      "Epoch 5/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.3115 - acc: 0.8723\n",
      "Epoch 6/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2927 - acc: 0.8759A: 0s - loss: 0.3080 - a\n",
      "Epoch 7/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.2814 - acc: 0.8723\n",
      "Epoch 8/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.2658 - acc: 0.8759\n",
      "Epoch 9/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.2537 - acc: 0.8796A: 1s - loss: 0.2657 - acc: 0\n",
      "Epoch 10/60\n",
      "274/274 [==============================] - 0s 883us/step - loss: 0.2371 - acc: 0.8905\n",
      "Epoch 11/60\n",
      "274/274 [==============================] - 0s 938us/step - loss: 0.2276 - acc: 0.8905\n",
      "Epoch 12/60\n",
      "274/274 [==============================] - 0s 934us/step - loss: 0.2176 - acc: 0.8942\n",
      "Epoch 13/60\n",
      "274/274 [==============================] - 0s 985us/step - loss: 0.1995 - acc: 0.9234\n",
      "Epoch 14/60\n",
      "274/274 [==============================] - 0s 883us/step - loss: 0.1893 - acc: 0.9161\n",
      "Epoch 15/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1852 - acc: 0.9197\n",
      "Epoch 16/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1716 - acc: 0.9234\n",
      "Epoch 17/60\n",
      "274/274 [==============================] - 0s 938us/step - loss: 0.1581 - acc: 0.9526\n",
      "Epoch 18/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1521 - acc: 0.9380\n",
      "Epoch 19/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1387 - acc: 0.9453\n",
      "Epoch 20/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1335 - acc: 0.9562\n",
      "Epoch 21/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1218 - acc: 0.9635\n",
      "Epoch 22/60\n",
      "274/274 [==============================] - 0s 985us/step - loss: 0.1140 - acc: 0.9708\n",
      "Epoch 23/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1050 - acc: 0.9708\n",
      "Epoch 24/60\n",
      "274/274 [==============================] - ETA: 0s - loss: 0.0886 - acc: 0.991 - 0s 883us/step - loss: 0.0915 - acc: 0.9854\n",
      "Epoch 25/60\n",
      "274/274 [==============================] - 0s 985us/step - loss: 0.0864 - acc: 0.9891\n",
      "Epoch 26/60\n",
      "274/274 [==============================] - 0s 975us/step - loss: 0.0810 - acc: 0.9854\n",
      "Epoch 27/60\n",
      "274/274 [==============================] - 0s 945us/step - loss: 0.0734 - acc: 0.9781\n",
      "Epoch 28/60\n",
      "274/274 [==============================] - 0s 938us/step - loss: 0.0664 - acc: 0.9818\n",
      "Epoch 29/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0630 - acc: 0.9854\n",
      "Epoch 30/60\n",
      "274/274 [==============================] - 0s 993us/step - loss: 0.0585 - acc: 0.9891\n",
      "Epoch 31/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0518 - acc: 0.9854\n",
      "Epoch 32/60\n",
      "274/274 [==============================] - 0s 945us/step - loss: 0.0441 - acc: 0.9854\n",
      "Epoch 33/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0439 - acc: 0.9891\n",
      "Epoch 34/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0339 - acc: 0.9891\n",
      "Epoch 35/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0366 - acc: 0.9927\n",
      "Epoch 36/60\n",
      "274/274 [==============================] - 0s 909us/step - loss: 0.0295 - acc: 0.9964\n",
      "Epoch 37/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0265 - acc: 0.9964\n",
      "Epoch 38/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0269 - acc: 0.9964\n",
      "Epoch 39/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0233 - acc: 0.9964\n",
      "Epoch 40/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0190 - acc: 0.9927- ETA: 0s - loss: 0.0065 - acc: 1.\n",
      "Epoch 41/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0151 - acc: 0.9964\n",
      "Epoch 42/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0148 - acc: 0.9964\n",
      "Epoch 43/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 44/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.9964\n",
      "Epoch 45/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 46/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.9964\n",
      "Epoch 47/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 48/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 49/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0081 - acc: 0.9964\n",
      "Epoch 50/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0047 - acc: 0.9964\n",
      "Epoch 54/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 55/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 9.8772e-04 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 9.5067e-04 - acc: 1.0000\n",
      "29/29 [==============================] - 1s 49ms/step\n",
      "Epoch 1/60\n",
      "274/274 [==============================] - 5s 17ms/step - loss: 0.5238 - acc: 0.7591\n",
      "Epoch 2/60\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 0.3669 - acc: 0.8248\n",
      "Epoch 3/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.3304 - acc: 0.8504\n",
      "Epoch 4/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.3068 - acc: 0.8796\n",
      "Epoch 5/60\n",
      "274/274 [==============================] - 0s 872us/step - loss: 0.2898 - acc: 0.8759\n",
      "Epoch 6/60\n",
      "274/274 [==============================] - 0s 839us/step - loss: 0.2737 - acc: 0.8832\n",
      "Epoch 7/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.2572 - acc: 0.8905\n",
      "Epoch 8/60\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 0.2455 - acc: 0.8832A: 0s - loss: 0.2356 - acc: 0. - ETA: 0s - loss: 0.1979 - acc\n",
      "Epoch 9/60\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.2278 - acc: 0.9088\n",
      "Epoch 10/60\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 0.2147 - acc: 0.9088\n",
      "Epoch 11/60\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.2017 - acc: 0.9124A: 0s - loss: 0.1955 - acc: 0.91\n",
      "Epoch 12/60\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 0.1950 - acc: 0.9343A: 2s - loss: 0.2390 - a - ETA: 1s - los\n",
      "Epoch 13/60\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 0.1776 - acc: 0.9270\n",
      "Epoch 14/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.1707 - acc: 0.9343\n",
      "Epoch 15/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.1626 - acc: 0.9197\n",
      "Epoch 16/60\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 0.1473 - acc: 0.9489\n",
      "Epoch 17/60\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 0.1381 - acc: 0.9526\n",
      "Epoch 18/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.1309 - acc: 0.9635A: 0s - loss: 0.1149 - ac\n",
      "Epoch 19/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.1177 - acc: 0.9708\n",
      "Epoch 20/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.1132 - acc: 0.9635\n",
      "Epoch 21/60\n",
      "274/274 [==============================] - 0s 956us/step - loss: 0.1016 - acc: 0.9708\n",
      "Epoch 22/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0936 - acc: 0.9781\n",
      "Epoch 23/60\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 0.0807 - acc: 0.9818A: 1s - loss: 0.1374 - \n",
      "Epoch 24/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0762 - acc: 0.9818A: 0s - loss: 0.0656 - acc: 0.\n",
      "Epoch 25/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0717 - acc: 0.9818\n",
      "Epoch 26/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0660 - acc: 0.9891\n",
      "Epoch 27/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0610 - acc: 0.9891\n",
      "Epoch 28/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0554 - acc: 0.9927\n",
      "Epoch 29/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0485 - acc: 0.9927\n",
      "Epoch 30/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0469 - acc: 0.9891\n",
      "Epoch 31/60\n",
      "274/274 [==============================] - 0s 865us/step - loss: 0.0440 - acc: 0.9927\n",
      "Epoch 32/60\n",
      "274/274 [==============================] - 0s 788us/step - loss: 0.0386 - acc: 0.9927\n",
      "Epoch 33/60\n",
      "274/274 [==============================] - 0s 869us/step - loss: 0.0373 - acc: 0.9891\n",
      "Epoch 34/60\n",
      "274/274 [==============================] - 0s 697us/step - loss: 0.0323 - acc: 0.9927\n",
      "Epoch 35/60\n",
      "274/274 [==============================] - 0s 693us/step - loss: 0.0315 - acc: 0.9891\n",
      "Epoch 36/60\n",
      "274/274 [==============================] - 0s 759us/step - loss: 0.0291 - acc: 0.9927\n",
      "Epoch 37/60\n",
      "274/274 [==============================] - 0s 661us/step - loss: 0.0256 - acc: 0.9927\n",
      "Epoch 38/60\n",
      "274/274 [==============================] - 0s 891us/step - loss: 0.0239 - acc: 0.9891\n",
      "Epoch 39/60\n",
      "274/274 [==============================] - 0s 792us/step - loss: 0.0209 - acc: 0.9927\n",
      "Epoch 40/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0197 - acc: 0.9927\n",
      "Epoch 41/60\n",
      "274/274 [==============================] - 0s 1ms/step - loss: 0.0221 - acc: 0.9927\n",
      "Epoch 42/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0143 - acc: 0.9927\n",
      "Epoch 43/60\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 0.0163 - acc: 0.9891A: 0s - loss: 0.0159 - acc:\n",
      "Epoch 44/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.9891\n",
      "Epoch 45/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0126 - acc: 0.9927\n",
      "Epoch 46/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0104 - acc: 0.9964\n",
      "Epoch 47/60\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 0.0126 - acc: 0.9964A: 0s - loss: 0.0127 - acc: 0.996\n",
      "Epoch 48/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0093 - acc: 0.9964A: 0s - loss: 0.0096 - acc: 0.996\n",
      "Epoch 49/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0111 - acc: 0.9964\n",
      "Epoch 50/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0073 - acc: 0.9964\n",
      "Epoch 51/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0071 - acc: 0.9964\n",
      "Epoch 52/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0079 - acc: 0.9964\n",
      "Epoch 53/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.0035 - acc: 1.0000A: 0s - loss: 0.0018 - acc: 1.00\n",
      "Epoch 55/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.0026 - acc: 1.0000\n",
      "29/29 [==============================] - 2s 63ms/step\n",
      "Model_Subclassing: 80.92% (8.02%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=Model_Subclassing, epochs=60, batch_size=5, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Model_Subclassing: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(150, activation = 'relu', input_shape = (13,)))\n",
    "network.add(layers.Dense(80, activation = 'relu'))\n",
    "network.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "network.compile(optimizer= 'adam',\n",
    "              loss ='binary_crossentropy',\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X) // k\n",
    "num_epochs = 100\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = Y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate([X[:i * num_val_samples],\n",
    "                                         X[(i + 1) * num_val_samples:]],axis=0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate([Y[:i * num_val_samples],\n",
    "                                            Y[(i + 1) * num_val_samples:]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "228/228 [==============================] - 6s 26ms/step - loss: 0.4662 - acc: 0.8070\n",
      "Epoch 2/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.2900 - acc: 0.8816A: 0s - loss: 0.2788 - acc: \n",
      "Epoch 3/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.2588 - acc: 0.8860\n",
      "Epoch 4/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.2362 - acc: 0.8947\n",
      "Epoch 5/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.2128 - acc: 0.9035\n",
      "Epoch 6/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.1967 - acc: 0.9079\n",
      "Epoch 7/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.1782 - acc: 0.9254\n",
      "Epoch 8/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.1590 - acc: 0.9386\n",
      "Epoch 9/100\n",
      "228/228 [==============================] - 0s 934us/step - loss: 0.1432 - acc: 0.9386\n",
      "Epoch 10/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.1308 - acc: 0.9605\n",
      "Epoch 11/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.1147 - acc: 0.9649\n",
      "Epoch 12/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.1050 - acc: 0.9781\n",
      "Epoch 13/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0922 - acc: 0.9781\n",
      "Epoch 14/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0741 - acc: 0.9868\n",
      "Epoch 15/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0640 - acc: 0.9868A: 0s - loss: 0.0340 - a\n",
      "Epoch 16/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0550 - acc: 0.9912\n",
      "Epoch 17/100\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 0.0464 - acc: 0.9868\n",
      "Epoch 18/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0395 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0371 - acc: 0.9912A: 0s - loss: 0.0243 - acc:\n",
      "Epoch 20/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0320 - acc: 0.9956\n",
      "Epoch 21/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0269 - acc: 0.9956\n",
      "Epoch 22/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0249 - acc: 0.9956\n",
      "Epoch 23/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0177 - acc: 0.9956\n",
      "Epoch 26/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0126 - acc: 0.9956\n",
      "Epoch 28/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0120 - acc: 0.9956\n",
      "Epoch 29/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0070 - acc: 1.0000A: 0s - loss: 0.0082 - acc: 1\n",
      "Epoch 33/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0066 - acc: 1.0000A: 0s - loss: 0.0082 - acc - ETA: 0s - loss: 0.0055 - acc: 1. - ETA: 0s - loss: 0.0070 - acc: 1.\n",
      "Epoch 34/100\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.0051 - acc: 1.0000A: 0s - loss: 0.0053 - acc: 1.000\n",
      "Epoch 36/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 1.0000A: 2s - loss: 0.0071 - acc: 1.0 - ETA: 0s - loss: 0.0054 - ac\n",
      "Epoch 37/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0026 - acc: 1.0000A: 0s - loss: 0.0027 - a\n",
      "Epoch 44/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0023 - acc: 1.0000 ETA: 0s - loss: 0.0015 - acc: 1\n",
      "Epoch 45/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0022 - acc: 1.0000A: 0s - loss: 0.0020 - acc: 1.00 - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.0\n",
      "Epoch 46/100\n",
      "228/228 [==============================] - 2s 9ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0017 - acc: 1.0000A: 0s - loss: 0.0018 - \n",
      "Epoch 50/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0018 - acc: 1.0000A: 2s - loss: 0.0021 - acc\n",
      "Epoch 51/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000ETA: 0s - loss: 0.0013 - acc: 1.0\n",
      "Epoch 55/100\n",
      "228/228 [==============================] - 0s 583us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "228/228 [==============================] - 0s 570us/step - loss: 0.0011 - acc: 1.0000ETA: 0s - loss: 0.0011 - acc: 1.000\n",
      "Epoch 57/100\n",
      "228/228 [==============================] - 0s 575us/step - loss: 0.0010 - acc: 1.0000ETA: 0s - loss: 0.0013 - acc: 1.0000  \n",
      "Epoch 58/100\n",
      "228/228 [==============================] - 0s 566us/step - loss: 9.5002e-04 - acc: 1.0000 0s - loss: 8.1197e-04 - acc: 1.000\n",
      "Epoch 59/100\n",
      "228/228 [==============================] - 0s 561us/step - loss: 8.9653e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.1840e-04 - acc: 1.0000A: 0s - loss: 8.4043e-04 - acc: 1.000\n",
      "Epoch 61/100\n",
      "228/228 [==============================] - 2s 9ms/step - loss: 8.2367e-04 - acc: 1.0000A: 0s - loss: 8.7279e-04 - acc: 1.000 - ETA: 0s - loss: 8.3977e-04 - acc: 1.00\n",
      "Epoch 62/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 8.1234e-04 - acc: 1.0000A: 2s - loss: 2.2361e-04 - a\n",
      "Epoch 63/100\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 7.6434e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 7.1486e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "228/228 [==============================] - 2s 10ms/step - loss: 6.6802e-04 - acc: 1.0000: 0s - loss: 6.6453e-04 - acc: 1.0\n",
      "Epoch 66/100\n",
      "228/228 [==============================] - 2s 10ms/step - loss: 6.5854e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 6.7579e-04 - acc: 1.0000A: 2s - loss: 9.9448e-04\n",
      "Epoch 68/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.7885e-04 - acc: 1.0000A: 0s - loss: 5.5951e-04 - acc: 1.\n",
      "Epoch 69/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.5664e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 5.2238e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.0590e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 4.7057e-04 - acc: 1.0000\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 2s 8ms/step - loss: 4.6822e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.2381e-04 - acc: 1.0000A: 0s - loss: 5.6336e-04 - acc:  - ETA: 0s - loss: 4.7679e-04 - acc\n",
      "Epoch 75/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 4.2405e-04 - acc: 1.0000A: 2s - loss: 7.3305e-04 - acc:\n",
      "Epoch 76/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 4.0237e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.8098e-04 - acc: 1.0000A: 0s - loss: 4.5118e-04 - ac\n",
      "Epoch 78/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 3.8518e-04 - acc: 1.0000A: 0s - loss: 2.6982e-04 - acc:  - ETA: 0s - loss: 3.1516e-04 - acc: 1.0 - ETA: 0s - loss: 2.9206e-04 - acc: 1.0\n",
      "Epoch 79/100\n",
      "228/228 [==============================] - 2s 10ms/step - loss: 3.6460e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 3.4371e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 3.1335e-04 - acc: 1.0000A: 0s - loss: 3.5830e-04 - a\n",
      "Epoch 82/100\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 3.0767e-04 - acc: 1.0000A: 1s - loss: 2.1129e-04 - acc: 1.0 - ETA: 0s - loss: 2.5363e-04 - acc: 1\n",
      "Epoch 83/100\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.1871e-04 - acc: 1.000 - 1s 3ms/step - loss: 3.0893e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "228/228 [==============================] - 0s 579us/step - loss: 2.9486e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 2.7985e-04 - acc: 1.0000A: 0s - loss: 2.7331e-04 - acc\n",
      "Epoch 86/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.6450e-04 - acc: 1.0000A: 0s - loss: 1.6975e-04 - acc: 1.0\n",
      "Epoch 87/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.5428e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.4603e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 2.3398e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.3535e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 2.2567e-04 - acc: 1.0000A: 0s - loss: 1.7398e-04 - acc: 1.000\n",
      "Epoch 92/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 2.1262e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.0720e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.9439e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.8656e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.8449e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 1.7712e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.6646e-04 - acc: 1.0000A: 0s - loss: 8.3798e-05 - a - ETA: 0s - loss: 1.6638e-04 - acc: 1.0\n",
      "Epoch 99/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.6697e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 1.6044e-04 - acc: 1.0000A: 0s - loss: 1.5713e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xecb3e48>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(partial_train_data,\n",
    "            partial_train_targets,\n",
    "            epochs= num_epochs,\n",
    "            batch_size =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = network.evaluate(val_data, val_targets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  4.007295951843262 \n",
      "Accuracy:  0.5200000011920929\n"
     ]
    }
   ],
   "source": [
    "print('Loss: ', loss, '\\nAccuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dropout_Model():\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    from keras.layers import Dropout\n",
    "    from keras.constraints import maxnorm\n",
    "    from keras.optimizers import SGD\n",
    "    \n",
    "    model= models.Sequential()\n",
    "    model.add(layers.Dense(60, kernel_constraint=maxnorm(3), input_dim=13, activation ='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(30, kernel_constraint=maxnorm(3), activation= 'relu' ))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, kernel_constraint=maxnorm(3),activation= 'sigmoid'))\n",
    "    \n",
    "    sgd =SGD(lr =0.01, momentum =0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(optimizer= sgd,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"heart.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[1:,0:-1].astype(float)\n",
    "Y = dataset[1:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X.mean(axis=0)\n",
    "X -= mean\n",
    "std = X.std(axis=0)\n",
    "X /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Stark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Dropout_Model: 80.55% (4.12%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=Dropout_Model, epochs=100, batch_size=10, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Dropout_Model: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second dropout model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Second_Dropout_Model():\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    from keras.layers import Dropout\n",
    "    from keras.constraints import maxnorm\n",
    "    from keras.optimizers import SGD\n",
    "    \n",
    "    model= models.Sequential()\n",
    "    model.add(layers.Dense(100, kernel_constraint=maxnorm(4), input_dim=13, activation ='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(60, kernel_constraint=maxnorm(4), activation= 'relu' ))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, kernel_constraint=maxnorm(4),activation= 'sigmoid'))\n",
    "    \n",
    "    sgd =SGD(lr =0.01, momentum =0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(optimizer= sgd,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second_Dropout_Model: 81.27% (8.92%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=Second_Dropout_Model, epochs=150, batch_size=10, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Second_Dropout_Model: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
